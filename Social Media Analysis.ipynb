{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc5e142",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b89b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import little_mallet_wrapper\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6de89",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ac986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.260000e+17</td>\n",
       "      <td>Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.260000e+17</td>\n",
       "      <td>@Apple will be adding more carrier support to the iPhone 4S (just announced)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.260000e+17</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love aff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.260000e+17</td>\n",
       "      <td>@RIM you made it too easy for me to switch to @Apple iPhone. See ya!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>1.260000e+17</td>\n",
       "      <td>I just realized that the reason I got into twitter was ios5 thanks @apple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment       TweetId  \\\n",
       "0  positive  1.260000e+17   \n",
       "1  positive  1.260000e+17   \n",
       "2  positive  1.260000e+17   \n",
       "3  positive  1.260000e+17   \n",
       "4  positive  1.260000e+17   \n",
       "\n",
       "                                                                                             TweetText  \n",
       "0             Now all @Apple has to do is get swype on the iphone and it will be crack. Iphone that is  \n",
       "1                         @Apple will be adding more carrier support to the iPhone 4S (just announced)  \n",
       "2  Hilarious @youtube video - guy does a duet with @apple 's Siri. Pretty much sums up the love aff...  \n",
       "3                                 @RIM you made it too easy for me to switch to @Apple iPhone. See ya!  \n",
       "4                            I just realized that the reason I got into twitter was ios5 thanks @apple  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"full-corpus-training.csv\")\n",
    "\n",
    "# Filter out records with the \"irrelevant\" label\n",
    "# df = df[df['Sentiment'] != 'irrelevant']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c73662",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bc1fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment    0\n",
       "TweetId      0\n",
       "TweetText    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019e3e9",
   "metadata": {},
   "source": [
    "### Preprocess and cleaning the text to remove any stop words or punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8595e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                          apple swype iphone crack iphone\n",
      "1                                            apple adding carrier support iphone announced\n",
      "2         hilarious youtube video duet apple siri pretty much sum love affair http exbnqjy\n",
      "3                                                            made easy switch apple iphone\n",
      "4                                                     realized reason twitter thanks apple\n",
      "5                    current blackberry user little disappointed move android apple iphone\n",
      "6    strangest thing siri said glad apple gave siri sense humor http cotwaeudbp happyplace\n",
      "7                                    great close personal event apple tonight regent store\n",
      "8                              company experience best customer service aside zappos apple\n",
      "9                                                                    apply apple hope call\n",
      "Name: CleanedTweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_text_data(text):\n",
    "    # preprocessor\n",
    "    text = p.clean(text)\n",
    "\n",
    "    # Remove HTML tags and URLs\n",
    "    text = re.sub(r'<[^>]+>|http[s]?://\\S+|http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and replace words with multiple consecutive letters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    # Insert a space before all capital letters in the middle of a sentence\n",
    "    text = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", text)\n",
    "\n",
    "    # Tokenize the tweet using TweetTokenizer\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    word_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Stop word removal and length filtering\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove('not')\n",
    "    filtered_text = [word for word in word_tokens if word.isalnum() and len(word) > 3 and word.lower() not in stop_words]\n",
    "\n",
    "    # Lowercase change\n",
    "    text = ' '.join(filtered_text).lower()\n",
    "\n",
    "    # Lemmatization using WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
    "    text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'TweetText' column\n",
    "df['CleanedTweet'] = df['TweetText'].apply(lambda x: little_mallet_wrapper.process_string(x, numbers='remove'))\n",
    "df['CleanedTweet'] = df['CleanedTweet'].apply(clean_text_data)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df['CleanedTweet'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584c08e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter             154.058752\n",
      "google              132.880067\n",
      "http                132.453139\n",
      "apple               118.209687\n",
      "microsoft           112.318714\n",
      "android              90.867818\n",
      "iphone               51.194151\n",
      "nexus                48.710277\n",
      "samsung              45.992413\n",
      "sandwich             43.026021\n",
      "cream                42.603218\n",
      "phone                40.175494\n",
      "galaxy               36.254216\n",
      "window               31.573166\n",
      "like                 29.438681\n",
      "siri                 28.974785\n",
      "facebook             28.959853\n",
      "ballmer              24.940511\n",
      "icecreamsandwich     24.068868\n",
      "steve                22.565562\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# TF IDF vectorizer with adjusted parameters\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "matrix_tfidf = tfidf_vect.fit_transform(df['CleanedTweet'])\n",
    "\n",
    "# Using get_feature_names_out\n",
    "featureNames = tfidf_vect.get_feature_names_out()\n",
    "\n",
    "# Data frame for our matrix_tfidf and featureNames\n",
    "df_tfidf = pd.DataFrame(data=matrix_tfidf.toarray(), columns=featureNames)\n",
    "\n",
    "# Adding up the importance scores (= TF-IDF scores) for every word.\n",
    "wordScores = df_tfidf.sum(axis=0)\n",
    "\n",
    "# Sorting words according to how much they matter in all the tweets\n",
    "# Sorting them with their overall TF-IDF scores.\n",
    "top20words = wordScores.sort_values(ascending=False).head(20)\n",
    "\n",
    "# Print top20words\n",
    "print(top20words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2328f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of top words\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# top20words.plot(kind='bar')\n",
    "# plt.title('Top 50 Words and Their TF-IDF Scores')\n",
    "# plt.xlabel('Words')\n",
    "# plt.ylabel('TF-IDF Score')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155ec33f",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ca18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['CleanedTweet'], df['Sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414952c",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37537411",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5281b",
   "metadata": {},
   "source": [
    "### Using Naive Bayes MultinomialNB to train our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341d0ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe72ca9",
   "metadata": {},
   "source": [
    "### Prediction and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255d7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.96      0.64      0.77       482\n",
      "    negative       1.00      0.01      0.03       138\n",
      "     neutral       0.63      0.99      0.77       680\n",
      "    positive       1.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.71      1395\n",
      "   macro avg       0.90      0.41      0.39      1395\n",
      "weighted avg       0.81      0.71      0.64      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165a18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d071a075",
   "metadata": {},
   "source": [
    "### Using other models to train our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca000688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification-Methods\n",
    "# Naïve Bayes\n",
    "# Voted Perceptron\n",
    "# Support Vector Machines\n",
    "# Decision Trees\n",
    "# K-nearest neighbor\n",
    "# Rocchio’s algorithm\n",
    "# Neural Networks\n",
    "\n",
    "# Predictive Performance Evaluation \n",
    "# Classification accuracy on a dataset\n",
    "# Recall\n",
    "# Precision\n",
    "# F-measure\n",
    "# Accuracy\n",
    "# Error and ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7eac15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e0548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5baf56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
